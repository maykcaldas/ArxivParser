{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy consists of two different concepts brought together:\n",
    "- DSPy programming language\n",
    "- DSPy compiler\n",
    "\n",
    "The DSPy programming language focus on organizing LMs as building blocks to create a more complex system. Here, LMs are treated as layers in a neural network, while their prompts are the weights.\n",
    "> In this context, different prompting techniques are seen as different layers: Predict (zero-shot), ChainOfThought, React, and so on.\n",
    "\n",
    "The DSPy compiler is a tool to optimize the DSPy program. The compiler uses a DSPy program, a training set, and a metric to optimize the program following a teleprompt (an optimizer)\n",
    "\n",
    "Therefore, the parallel follows:\n",
    "\n",
    "| DSPy              | PyTorch                   |\n",
    "| :--------:        | :-------:                 |\n",
    "| LMs               | Layers                    |\n",
    "| LM prompts        | Layers' weights           |\n",
    "| Teleprompters     | Optimizer                 |\n",
    "| Compiling         | Training                  |\n",
    "| Bootstraping      | Hyperparameters tuning    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-on!\n",
    "\n",
    "Despite DSPy's architecture, we can still call LMs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "# gpt35_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', model_type=\"text\")\n",
    "dspy.settings.configure(lm=gpt35_turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Output ----------------------------------------\n",
      "['\\n\\nYes, you can call LM (linear model) models directly in DSPy using the LinearModel class. This class allows you to create and manipulate linear models, including fitting them to data, making predictions, and evaluating model performance.', '\\n\\nNo, DSPy does not have the capability to directly call LM models. DSPy is a library for digital signal processing and does not have any specific functionality for LM models. However, you can use DSPy to preprocess data and then use a separate library or tool to call LM models.', '\\n\\nNo, DSPy does not have a direct interface for calling LM models. It is designed to be used for building and training neural networks, not for directly calling pre-trained models. However, DSPy does have the ability to load and use pre-trained models as part of the network architecture, so you could use it in that way if you have a pre-trained LM model that you want to incorporate into your network.', '\\n\\nYes, DSPy allows you to call LM models directly by importing them from the \"lm\" module. For example, you can use the following code to import and call the linear regression model:\\n\\n```\\nfrom dspy.lm import LinearRegression\\n\\n# create an instance of the model\\nmodel = LinearRegression()\\n\\n# fit the model to your data\\nmodel.fit(X, y)\\n\\n# make predictions\\npredictions = model.predict(X_test)\\n```\\n\\nYou can also import and use other LM models such as Ridge, Lasso, and ElasticNet in a similar manner. ', '\\n\\nYes, you can call LM models directly using DSPy. DSPy provides a variety of functions and classes for working with LM models, including loading, fitting, and evaluating models. You can also use DSPy to manipulate and visualize the data used in the models.']\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'n' : 5,\n",
    "    'max_tokens' : 250,\n",
    "    'temperature' : 0.5,\n",
    "}\n",
    "h = f\"{40*'-'} Output {40*'-'}\"\n",
    "print(f\"{h}\\n\"\n",
    "      f\"{gpt35_turbo('Using DSPy, can I just call LM models directly?', **config)}\"\n",
    "      f\"\\n{len(h)*'-'}\\n\"\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we dont need DSPy for that. DSPy strength comes from wrapping LMs in programming modules (layers) that can be assambled to create programs (models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Predict output ----------------------------------------\n",
      "['Paris', 'Paris', 'Paris', 'Paris', 'Paris']\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------- Chain of Thoughts output -----------------------------------\n",
      "['Paris', 'Paris', 'Paris', 'The capital of France is Paris.', 'Paris']\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pred = dspy.Predict('question -> answer', **config)\n",
    "\n",
    "h = f\"{40*'-'} Predict output {40*'-'}\"\n",
    "print(f\"{h}\\n\"\n",
    "      f\"{pred(question = 'What is the capital of France?').completions.answer}\"\n",
    "      f\"\\n{len(h)*'-'}\\n\"\n",
    "      )\n",
    "\n",
    "CoT = dspy.ChainOfThought('question -> answer', **config)\n",
    "h = f\"{35*'-'} Chain of Thoughts output {35*'-'}\"\n",
    "print(f\"{h}\\n\"\n",
    "      f\"{CoT(question = 'What is the capital of France?').completions.answer}\"\n",
    "      f\"\\n{len(h)*'-'}\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know which LM are `dspy.Predict` and `dspy.ChainOfThought using`? \n",
    "\n",
    "Annoyingly, DSPy takes the LM from its configuration (`dspy.settings.configure(lm=gpt35_turbo)`), not from the modules. \n",
    "\n",
    "But there's a workaround: `dspy.context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Predict output ----------------------------------------\n",
      "['Question: What is the capital of France?\\nAnswer: The capital of France is Paris.', 'Question: What is the capital of France?\\nAnswer: The capital of France is Paris.']\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt4_turbo = dspy.OpenAI(model='gpt-4-1106-preview')\n",
    "\n",
    "with dspy.context(lm=gpt4_turbo):\n",
    "  config = {\n",
    "      'n' : 2,\n",
    "      'max_tokens' : 250,\n",
    "      'temperature' : 0.5,\n",
    "  }\n",
    "  pred = dspy.Predict('question -> answer', **config)\n",
    "\n",
    "  h = f\"{40*'-'} Predict output {40*'-'}\"\n",
    "  print(f\"{h}\\n\"\n",
    "        f\"{pred(question = 'What is the capital of France?').completions.answer}\"\n",
    "        f\"\\n{len(h)*'-'}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy supports a few remote LMs:\n",
    "- OpenAI\n",
    "- AzureOpenAI\n",
    "- Cohere\n",
    "- Anyscale\n",
    "- Together\n",
    "- Databricks\n",
    "- Mistral\n",
    "- dspy.AWSMistral\n",
    "- dspy.AWSAnthropic\n",
    "- dspy.AWSMeta\n",
    "- ...\n",
    "\n",
    "But we can also use local LMs using a local server with:\n",
    "- HFClientTGI\n",
    "- HFClientVLLM\n",
    "- HFModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSM8K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dspy.teleprompt import (\n",
    "    LabeledFewShot,\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    "    Ensemble\n",
    ")\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "gpt35_turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "dspy.settings.configure(lm=gpt35_turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 67816.78it/s]\n",
      "\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 68176.11it/s]\n"
     ]
    }
   ],
   "source": [
    "gsm8k = GSM8K()\n",
    "gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Megan pays $16 for a shirt that costs $22 before sales. What is the amount of the discount?\n",
      "Answer: 6\n"
     ]
    }
   ],
   "source": [
    "q=8\n",
    "print(\n",
    "    f'Question: {gsm8k_trainset[q].question}\\n'\n",
    "    f'Answer: {gsm8k_trainset[q].answer}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5 turbo with no fancy prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.predict(question=question)\n",
    "\n",
    "vanilla = Vanilla()\n",
    "vanilla(question=gsm8k_trainset[8].question)\n",
    "\n",
    "# Compiling\n",
    "fewshot = LabeledFewShot(k=8).compile(vanilla, trainset=gsm8k_trainset)\n",
    "\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric)\n",
    "bootstrap = tp.compile(vanilla , trainset=gsm8k_trainset , valset=gsm8k_devset)\n",
    "bootstrap2 = tp.compile(vanilla , teacher=bootstrap, trainset=gsm8k_trainset, valset=gsm8k_devset)\n",
    "# ensemble = Ensemble(reduce_fn=dspy.majority).compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking it for a random question: \n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "Label:      47\n",
      "zeroshot:   32\n",
      "fewshot:    47\n",
      "bootstrap:  47\n",
      "bootstrap2: 47\n",
      "\n",
      "Now, a systematic evaluation\n",
      "Average Metric: 0 / 10  (0.0%)\n",
      "Average Metric: 6 / 10  (60.0%)\n",
      "Average Metric: 10 / 10  (100.0%)\n",
      "Average Metric: 10 / 10  (100.0%)\n",
      "\n",
      "Scores:zeroshot:   0.0\n",
      "fewshot:    60.0\n",
      "bootstrap:  100.0\n",
      "bootstrap2: 100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checking it for a random question: \\nQuestion: {gsm8k_devset[q].question}\")\n",
    "print(f\"Label:      {gsm8k_devset[q].answer}\\n\"\n",
    "      f\"zeroshot:   {vanilla(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"fewshot:    {fewshot(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap:  {bootstrap(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap2: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      # f\"Ensemble: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      )\n",
    "\n",
    "\n",
    "print(\"Now, a systematic evaluation\")\n",
    "evaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=4, display_progress=True, display_table=0)\n",
    "print(f\"\\nScores:\\n\"\n",
    "      f\"zeroshot:   {evaluate(vanilla)}\\n\"\n",
    "      f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "      f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "      f\"bootstrap2: {evaluate(bootstrap2)}\\n\"\n",
    "      # f\"Ensemble: {evaluate(ensamble)}\\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5 turbo with ChainOfThought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cot = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.cot(question=question)\n",
    "\n",
    "cot = CoT()\n",
    "\n",
    "# Compiling\n",
    "fewshot = LabeledFewShot(k=8).compile(cot, trainset=gsm8k_trainset)\n",
    "\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric)\n",
    "bootstrap = tp.compile(cot , trainset=gsm8k_trainset , valset=gsm8k_devset)\n",
    "bootstrap2 = tp.compile(cot , teacher=bootstrap, trainset=gsm8k_trainset, valset=gsm8k_devset)\n",
    "# ensemble = Ensemble(reduce_fn=dspy.majority).compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 47\n",
      "fewshot: 47\n",
      "bootstrap: 47\n",
      "bootstrap2: 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checking it for a random question: \\nQuestion: {gsm8k_devset[q].question}\")\n",
    "print(f\"Label:      {gsm8k_devset[q].answer}\\n\"\n",
    "      f\"zeroshot:   {cot(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"fewshot:    {fewshot(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap:  {bootstrap(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap2: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      # f\"Ensemble: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      )\n",
    "\n",
    "print(\"Now, a systematic evaluation\")\n",
    "evaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=4, display_progress=True, display_table=0)\n",
    "print(f\"\\nScores:\\n\"\n",
    "      f\"zeroshot:   {evaluate(cot)}\\n\"\n",
    "      f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "      f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "      f\"bootstrap2: {evaluate(bootstrap2)}\\n\"\n",
    "      # f\"Ensemble: {evaluate(ensamble)}\\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5 turbo with with CoT and reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThoughtReflection(dspy.Module): \n",
    "    def __init__(self , num_attempts):\n",
    "        self.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts)\n",
    "        self.compare = dspy.MultiChainComparison('question -> answer', M=num_attempts)\n",
    "        \n",
    "    def forward(self , question):\n",
    "        completions = self.predict(question=question).completions\n",
    "        return self.compare(question=question , completions=completions)\n",
    "\n",
    "reflection = ThoughtReflection(num_attempts=5)\n",
    "\n",
    "# Compiling\n",
    "fewshot = LabeledFewShot(k=8).compile(reflection, trainset=gsm8k_trainset)\n",
    "\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric)\n",
    "bootstrap = tp.compile(reflection , trainset=gsm8k_trainset , valset=gsm8k_devset)\n",
    "bootstrap2 = tp.compile(reflection , teacher=bootstrap, trainset=gsm8k_trainset, valset=gsm8k_devset)\n",
    "# ensemble = Ensemble(reduce_fn=dspy.majority).compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label: {gsm8k_devset[q].answer}\\n\"\n",
    "      f\"zeroshot: {reflection(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"fewshot: {fewshot(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap: {bootstrap(gsm8k_devset[q].question).answer}\\n\"\n",
    "      f\"bootstrap2: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      # f\"Ensemble: {bootstrap2(gsm8k_devset[q].question).answer}\\n\"\n",
    "      )\n",
    "\n",
    "print(\"Now, a systematic evaluation\")\n",
    "evaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=4, display_progress=True, display_table=0)\n",
    "print(f\"\\nScores:\\n\"\n",
    "      f\"zeroshot:   {evaluate(reflection)}\\n\"\n",
    "      f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "      f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "      f\"bootstrap2: {evaluate(bootstrap2)}\\n\"\n",
    "      # f\"Ensemble: {evaluate(ensamble)}\\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that BootstrapFewShot is not an optimizing teleprompter, i.e. it simple creates and validates examples for steps of the pipeline but does not optimize the metric. \n",
    "\n",
    "Other teleprompters like BootstrapFewShotWithRandomSearch and MIPRO will apply direct optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HotPotQA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HotPotQA is a multi-hop QA dataset. \n",
    "\n",
    "They used CoT, ReAct, and multihop as model pipelines. And compiled it with bootstrap and finetuned T5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets.hotpotqa import HotPotQA\n",
    "from dspy.teleprompt import (\n",
    "    LabeledFewShot,\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    "    BootstrapFinetune,\n",
    "    Ensemble\n",
    ")\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.evaluate import answer_exact_match\n",
    "\n",
    "gpt35_turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=gpt35_turbo, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HotPotQA(train_size=10, dev_size=10)\n",
    "hotpot_trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "hotpot_devset = [x.with_inputs('question') for x in dataset.dev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.predict(question=question)\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self , num_passages=3):\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question  -> answer\")\n",
    "\n",
    "    def forward(self , question):\n",
    "        context = self.retrieve(question).passages\n",
    "        return self.generate_answer(context=context, question=question)\n",
    "\n",
    "\n",
    "class React(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.react = dspy.ReAct(\"question  -> answer\", tools=[dspy.Retrieve(k=1)], max_iters=5)\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.react(question=question)\n",
    "\n",
    "\n",
    "class BasicMultiHop(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3):\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = dspy.ChainOfThought(\"context , question  -> search_query\")\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context , question  -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        for  hop in  range (2):\n",
    "            query = self.generate_query(context=context, question=question).search_query \n",
    "            context  += self.retrieve(query).passages\n",
    "\n",
    "        return self.generate_answer(context=context, question=question)\n",
    "\n",
    "# Pipelines\n",
    "vanilla = Vanilla()\n",
    "cot = RAG(num_passages=5)\n",
    "react = React()\n",
    "multihop = BasicMultiHop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to train 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0): 100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0%)\n",
      "Score: 10.0 for set: [0]\n",
      "New best score: 10.0 for seed -3\n",
      "Scores so far: [10.0]\n",
      "Best score: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "New best score: 20.0 for seed -2\n",
      "Scores so far: [10.0, 20.0]\n",
      "Best score: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "New best score: 30.0 for seed -1\n",
      "Scores so far: [10.0, 20.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.5\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:09,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:03<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.6\n",
      "Average of max per entry across top 8 scores: 0.6\n",
      "Average of max per entry across top 9999 scores: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:10,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.8\n",
      "Average of max per entry across top 8 scores: 0.8\n",
      "Average of max per entry across top 9999 scores: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.8\n",
      "Average of max per entry across top 8 scores: 0.8\n",
      "Average of max per entry across top 9999 scores: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 0.9\n",
      "Average of max per entry across top 9999 scores: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:12<00:01,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  30%|███       | 3/10 [00:01<00:03,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0):  50%|█████     | 5/10 [00:03<00:03,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:05<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 7  (14.3):  70%|███████   | 7/10 [00:07<00:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 8  (25.0):  80%|████████  | 8/10 [00:09<00:02,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 3 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:04<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  40%|████      | 4/10 [00:02<00:03,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0):  50%|█████     | 5/10 [00:04<00:04,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:06<00:05,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 7  (14.3):  70%|███████   | 7/10 [00:08<00:04,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 8  (12.5):  80%|████████  | 8/10 [00:10<00:03,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:16<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:08<00:05,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 6  (33.3):  60%|██████    | 6/10 [00:02<00:01,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 7  (42.9):  70%|███████   | 7/10 [00:03<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 7  (28.6):  70%|███████   | 7/10 [00:04<00:02,  1.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  40%|████      | 4/10 [00:01<00:01,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:03<00:02,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  40%|████      | 4/10 [00:02<00:02,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:03<00:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 7  (14.3):  70%|███████   | 7/10 [00:05<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 8  (25.0):  80%|████████  | 8/10 [00:06<00:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0):  50%|█████     | 5/10 [00:03<00:03,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:05<00:04,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 7  (14.3):  70%|███████   | 7/10 [00:08<00:05,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:09<00:06,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 4  (50.0):  40%|████      | 4/10 [00:03<00:04,  1.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 5  (40.0):  50%|█████     | 5/10 [00:04<00:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 6  (50.0):  60%|██████    | 6/10 [00:06<00:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 8  (37.5):  80%|████████  | 8/10 [00:10<00:03,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.1 seconds after 3 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "Score: 30.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:09,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0):  50%|█████     | 5/10 [00:01<00:01,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 6  (16.7):  60%|██████    | 6/10 [00:02<00:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 7  (28.6):  70%|███████   | 7/10 [00:03<00:02,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:06<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:07<00:07,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  40%|████      | 4/10 [00:02<00:02,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0):  50%|█████     | 5/10 [00:03<00:03,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 6  (33.3):  60%|██████    | 6/10 [00:04<00:03,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 7  (28.6):  70%|███████   | 7/10 [00:05<00:02,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x118e89b40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:10<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n",
      "Score: 20.0 for set: [10]\n",
      "Scores so far: [10.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 20.0, 20.0, 30.0, 30.0, 30.0, 30.0, 20.0, 30.0, 20.0, 20.0]\n",
      "Best score: 30.0\n",
      "Average of max per entry across top 1 scores: 0.3\n",
      "Average of max per entry across top 2 scores: 0.4\n",
      "Average of max per entry across top 3 scores: 0.6\n",
      "Average of max per entry across top 5 scores: 0.9\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n",
      "19 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compilations\n",
    "for model in [cot]:#, ract, multihop]:\n",
    "    fewshot = LabeledFewShot(k=8).compile(model, trainset=hotpot_trainset)\n",
    "\n",
    "    tp = BootstrapFewShotWithRandomSearch(metric=answer_exact_match)\n",
    "    bootstrap = tp.compile(model, trainset=hotpot_trainset, valset=hotpot_devset)\n",
    "    # multihop_t5 = BootstrapFinetune(metric=answer_exact_match).compile(model, teacher=bootstrap, trainset=hotpot_trainset, target='t5-large')\n",
    "\n",
    "    # evaluate = Evaluate(devset=hotpot_devset, metric=answer_exact_match, num_threads=4, display_progress=True, display_table=0)\n",
    "    # print(f\"\\nScores for {model}:\\n\"\n",
    "    #     f\"zeroshot:   {evaluate(model)}\\n\"\n",
    "    #     f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "    #     f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "    #     # f\"T5-FT: {evaluate(multihop_t5)}\\n\"\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowell Thomas George\n",
      "Lowell George\n",
      "Given the fields `context`, `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Bill Payne | Bill Payne (born March 12, 1949) is an American pianist who co-founded with Lowell George the American rock band Little Feat. He is considered by many other rock pianists, including Elton John, to be one of the finest American piano rock and blues musicians. In addition to his trademark barrelhouse blues piano, he is noted for his work on the Hammond B3 organ. Payne is an accomplished songwriter whose credits include \"Oh, Atlanta\". Following the death of Little Feat drummer Richie Hayward on August 12, 2010, Payne is the only member of the group from the original four-piece line-up currently playing in the band.»\n",
      "[2] «Lowell Mason | Lowell Mason (January 8, 1792 – August 11, 1872) was a leading figure in American church music, the composer of over 1600 hymn tunes, many of which are often sung today. His most well-known tunes include his arrangement of \"Joy to the World\" and \"Bethany\", his setting of the hymn, \"Nearer, My God, to Thee\". He was largely responsible for introducing music into American public schools, and is considered to be the first important music educator in the United States. He is also widely criticized for his role in helping to largely eliminate the robust tradition of participatory sacred music that flourished in America before his time.»\n",
      "[3] «Thanks, I'll Eat It Here | Thanks, I'll Eat It Here is the title of the only solo album by the late rock and roll singer-songwriter Lowell George. While George is best known for his work with Little Feat, by 1977 Lowell felt that they were moving increasingly into jazz-rock, a form in which he felt little interest. As a result, he began working on his own album. \"Thanks, I'll Eat It Here\" is an eclectic mix of styles reminiscent of Little Feat's earlier albums - in particular \"Dixie Chicken\", on which the track \"Two Trains\" originally appeared. The album was released just before the death of Lowell George in 1979 and has cover art by Neon Park (a feature of almost all Little Feat albums) containing several pop-/cult references including a picnic scene, mirroring Édouard Manet's \"Le déjeuner sur l'herbe\", which shows Bob Dylan, Fidel Castro and Marlene Dietrich as Der Blaue Engel with an open copy of \"Howl\" beside them.»\n",
      "\n",
      "Question: which of the two musician has has the highest number of musical skill Stza or Lowell George\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      " determine which of the two musicians has the highest number of musical skills. First, we need to look at the context and gather information about each musician's musical abilities. Then, we can compare the information and make a conclusion.\n",
      "\n",
      "Answer: Based on the context, it can be determined that Lowell George has a higher number of musical skills. He is considered by many to be one of the finest American piano rock and blues musicians, and is also noted for his work on the Hammond B3 organ. In addition, he is an accomplished songwriter with over 1600 hymn tunes to his credit. On the other hand, there is no mention of Stza's musical abilities in the given context. Therefore, it can be concluded that Lowell George has a\n"
     ]
    }
   ],
   "source": [
    "print(hotpot_devset[0].answer)\n",
    "print(bootstrap(hotpot_devset[0].question).answer)\n",
    "\n",
    "print(gpt35_turbo.history[0]['prompt'])\n",
    "print(gpt35_turbo.history[0]['response']['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0): 100%|██████████| 10/10 [00:00<00:00, 2353.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0): 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 10  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:00<00:00, 1026.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 10  (30.0%)\n",
      "\n",
      "Scores for cot:\n",
      "zeroshot:   10.0\n",
      "fewshot:    20.0\n",
      "bootstrap:  30.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate = Evaluate(devset=hotpot_devset, metric=answer_exact_match, num_threads=4, display_progress=True, display_table=0)\n",
    "print(f\"\\nScores for cot:\\n\"\n",
    "    f\"zeroshot:   {evaluate(cot)}\\n\"\n",
    "    f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "    f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "    # f\"T5-FT: {evaluate(multihop_t5)}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do something fun now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from dspy.teleprompt import (\n",
    "    LabeledFewShot,\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    ")\n",
    "\n",
    "gpt35_turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "dspy.settings.configure(lm=gpt35_turbo)\n",
    "\n",
    "df = pd.read_csv(\"../data/12744_ocm_dataset.csv\", sep=\";\").astype(str)\n",
    "df = df.rename(columns={\"prompt\": \"procedure\", \"completion\": \"C2_yield\"})\n",
    "\n",
    "# Create dataset\n",
    "dataset = [dspy.Example(x).with_inputs('procedure') for x in df.to_dict(orient='records')]\n",
    "train_dataset = dataset[:1000]\n",
    "dev_dataset = random.sample(dataset[1000:], 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(example, pred, trace=None):\n",
    "    try:\n",
    "        pred = np.array(pred)\n",
    "        labels = np.array([float(exp.C2_yield) for exp in example])\n",
    "    except: \n",
    "        return -10\n",
    "    return -np.sum(np.absolute(pred - labels)) / len(pred)\n",
    "\n",
    "class OCMSignature(dspy.Signature):\n",
    "    \"\"\"Predict C2 yield from a given experimental procedure\"\"\"\n",
    "\n",
    "    # context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    procedure = dspy.InputField(desc=\"A description of the experiment\")\n",
    "    answer = dspy.OutputField(desc=\"C2 yield in %\")\n",
    "\n",
    "\n",
    "class Vanilla(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(\"procedure -> C2_yield\")\n",
    "        # self.predict = dspy.Predict(OCMSignature)\n",
    "    \n",
    "    def forward(self, procedure):\n",
    "        return self.predict(procedure=procedure)\n",
    "\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self , num_passages=3):\n",
    "        self.cot = dspy.ChainOfThought(\"procedure  -> C2_yield\")\n",
    "\n",
    "    def forward(self , procedure):\n",
    "        return self.cot(procedure=procedure)\n",
    "\n",
    "vanilla = Vanilla()\n",
    "cot = CoT()\n",
    "\n",
    "fewshot = LabeledFewShot(k=5).compile(vanilla, trainset=train_dataset)\n",
    "\n",
    "tp = BootstrapFewShotWithRandomSearch(metric=mae)\n",
    "bootstrap = tp.compile(fewshot, trainset=train_dataset, valset=dev_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'procedure': 'To synthesize Na2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consiting of n.a. (0%), Na (67%), W (33%), at 50 ºC for 6 h. Once activated the reaction is ran at 900 ºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.4 mL/min, O2: 1.1 mL/min), leading to a contact time of 0.5 s.', 'C2_yield': '5.82'}) (input_keys={'procedure'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    C2_yield='n.a.'\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dev_dataset[0])\n",
    "bootstrap(dev_dataset[0].procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `procedure`, produce the fields `C2_yield`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Procedure: ${procedure}\n",
      "C 2 Yield: ${C2_yield}\n",
      "\n",
      "---\n",
      "\n",
      "Procedure: To synthesize Na2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consiting of n.a. (0%), Na (67%), W (33%), at 50 ºC for 6 h. Once activated the reaction is ran at 900 ºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.4 mL/min, O2: 1.1 mL/min), leading to a contact time of 0.5 s.\n",
      "C 2 Yield:\u001b[32m n.a.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt35_turbo.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, devset, metric, verbose=False):\n",
    "    pred = []\n",
    "    examples = []\n",
    "    failed = 0\n",
    "    for example in devset:\n",
    "        try:\n",
    "            yhat = model(example.procedure).C2_yield\n",
    "            pred.append(float(yhat))\n",
    "            examples.append(example)\n",
    "        except:\n",
    "            failed += 1\n",
    "            if verbose:\n",
    "                print(\"WARNING: Failed to parse respose for example:\", example)\n",
    "                print(\"WARNING: Model completed:\", yhat, \"\\n\")\n",
    "    return metric(examples, pred), pred, examples, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, yhat, y, f = evaluate(bootstrap, dev_dataset[:2], mae)\n",
    "print(m, f)\n",
    "yhat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=dev_dataset, metric=dspy.bleu, num_threads=4, display_progress=True, display_table=0)\n",
    "print(f\"\\nScores for {cot}:\\n\"\n",
    "    f\"zeroshot:   {evaluate(cot)}\\n\"\n",
    "    f\"fewshot:    {evaluate(fewshot)}\\n\"\n",
    "    # f\"bootstrap:  {evaluate(bootstrap)}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=dev_dataset, metric=answer_exact_match, num_threads=4, display_progress=True, display_table=0, )\n",
    "print(f\"\\nScores for {cot}:\\n\"\n",
    "      f\"vanilla:  {evaluate(vanilla)}\\n\"\n",
    "      f\"zeroshot: {evaluate(cot)}\\n\"\n",
    "    #   f\"fewshot:  {evaluate(fewshot)}\\n\"\n",
    "    #   f\"bootstrap:{evaluate(bootstrap)}\\n\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
